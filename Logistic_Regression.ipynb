{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a powerful classification algorithm that is widely used for binary classification tasks. It models the probability that a given input belongs to a certain class. In logistic regression, the output is transformed using the sigmoid function to produce values between 0 and 1, which can be interpreted as class probabilities.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Given a dataset with $N$ samples and $M$ features, the goal of logistic regression is to find the optimal set of weights $\\theta$ that best separates the two classes. The mathematical formulation of logistic regression involves the following components:\n",
    "\n",
    "- Inputs: $X \\in \\mathbb{R}^{M \\times N+1}$ (matrix where each row represents a sample and each column corresponds to a feature plus a column of ones for bias term)\n",
    "- Weights: $\\theta \\in \\mathbb{R}^{N+1}$ (a vector of coefficients plus bias term)\n",
    "- Logits: $t = X\\theta$ ($t \\in \\mathbb{R}^{N+1}$)\n",
    "\n",
    "The assumption of logistic regression is that logit $\\log \\dfrac{p(x)}{1-p(x)}$ is linear function of $x$:\n",
    "\n",
    "$$\\log \\dfrac{p(x)}{1 - p(x)} = \\theta^T x$$\n",
    "\n",
    "Solving for $p$, we get:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\dfrac{p(x)}{1 - p(x)} = e^{\\theta^T x}\\\\\n",
    "p(x) = (1 - p(x))e^{\\theta^T x}\\\\\n",
    "p(x) = e^{\\theta^T x} - p(x)e^{\\theta^T x}\\\\\n",
    "p(x)(1 + e^{\\theta^T x}) = e^{\\theta^T x}\\\\\n",
    "p(x) = \\dfrac{e^{\\theta^T x}}{(1 + e^{\\theta^T x})}\\\\\n",
    "p(x) = \\dfrac{1}{(1 + e^{-\\theta^T x})}\\\\\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "From the equation above we can see that, $P(Y=1|X=x) = \\sigma(t)$ , where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "The sigmoid function $\\sigma(t)$ is defined as:\n",
    "$$ \\sigma(t) = \\frac{1}{1 + e^{-t}} $$\n",
    "\n",
    "The likelihood of the observed data under the logistic regression model is given by:\n",
    "$$ L(\\theta) = \\prod_{i=1}^{N} P(Y=y_i|X=x_i) = \\prod_{i=1}^{N} \\left(\\sigma(\\theta^T x_i)\\right)^{y_i} \\left(1 - \\sigma(\\theta^T x_i)\\right)^{1 - y_i} $$\n",
    "\n",
    "The goal is to maximize the likelihood, which is equivalent to minimizing the negative log-likelihood (log loss):\n",
    "$$ J(\\theta) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log\\left(\\sigma(\\theta^T x_i)\\right) + (1 - y_i) \\log\\left(1 - \\sigma(\\theta^T x_i)\\right) \\right] $$\n",
    "\n",
    "## Optimization\n",
    "\n",
    "Optimizing the logistic regression model involves finding the values of $W$ and $b$ that minimize the log loss $J(W, b)$. This is typically done using optimization algorithms like gradient descent or its variants. The gradients of the log loss with respect to $W$ and $b$ can be computed using the chain rule and are used to update the model parameters iteratively.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Logistic regression is a fundamental classification algorithm with a well-defined mathematical formulation. It models class probabilities using the sigmoid function and optimizes its parameters to minimize the log loss. This approach allows logistic regression to make accurate predictions for binary classification tasks.\n",
    "\n",
    "Logistic Regression is a widely used classification algorithm in machine learning. Unlike linear regression, which predicts continuous values, logistic regression predicts the probability of an input belonging to a particular class. The logistic function, also known as the sigmoid function, is used to model this probability.\n",
    "\n",
    "## Loss Function and Derivative\n",
    "\n",
    "In logistic regression, the sigmoid function is applied to the linear combination of input features, resulting in the predicted probability. The loss function used in logistic regression is the Cross-Entropy Loss, also known as Log Loss or Binary Cross-Entropy. The loss measures the difference between the predicted probabilities and the actual labels.\n",
    "\n",
    "Given:\n",
    "- Inputs: $X$ (matrix of features)\n",
    "- Weights: $W$\n",
    "- Predicted probabilities: $s = \\sigma(X\\theta)$, where $\\sigma$ is the sigmoid function\n",
    "- Actual labels: $Y$ (binary, 0 or 1)\n",
    "\n",
    "The Cross-Entropy Loss ($J$) is defined as the negative log-likelihood of the data given the model's predictions:\n",
    "$$ J(\\theta) = \\sum_{n=1}^{N} \\left( y_n \\log \\left(s_n\\right) + \\left(1 - y_n\\right) \\log \\left(1 - s_n\\right) \\right) $$\n",
    "\n",
    "Where:  \n",
    "\n",
    "$t(\\theta) = \\theta^T x$ \n",
    " \n",
    "$s(t) = \\sigma (t)$  \n",
    "  \n",
    "$\\dfrac{d s(t)}{dt} = \\sigma(t)(1-\\sigma(t)) = s(t)(1-s(t))$\n",
    "\n",
    "$\\dfrac{d t}{d \\theta} = x$\n",
    "\n",
    "Now defferentiate:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\dfrac{\\partial J(\\theta_i)}{\\partial \\theta} &=  -\\sum_{n=1}^{N} \\left(y_n  \\cdot \\dfrac{\\partial  \\log (s_n)}{\\partial s_n} \\cdot \\dfrac{\\partial s_n(t_n)}{\\partial t_n}  \\cdot \\dfrac{\\partial t_n(\\theta)}{\\partial \\theta_i} + (1-y_n)  \\cdot \\dfrac{\\partial  \\log (1-s_n)}{\\partial s_n}  \\cdot \\dfrac{\\partial s_n(t_n)}{\\partial t_n}  \\cdot \\dfrac{\\partial t_n(\\theta)}{\\partial \\theta_i}\\right)\\\\\n",
    "&= -\\sum_{n=1}^{N} \\left( \\dfrac{y_n}{s_n} \\cdot s_n \\cdot (1-s_n) \\cdot x_n + \\dfrac{1-y_n}{1-s_n} \\cdot (-1) \\cdot s_n \\cdot (1-s_n) \\cdot x_n \\right) \\\\\n",
    "&= -\\sum_{n=1}^{N} \\left( y_n\\cdot (1-s_n) \\cdot x_n + (y_n-1)\\cdot s_n  \\cdot x_n \\right) \\\\\n",
    "&= -\\sum_{n=1}^{N} \\left(y_n \\cdot x_n - y_n \\cdot s_n \\cdot x_n + y_n \\cdot s_n \\cdot x_n - s_n \\cdot x_n \\right) \\\\\n",
    "&= -\\sum_{n=1}^{N} \\left((y_n - s_n) \\cdot x_n \\right) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Removing summation gives us the derivative in vector form:\n",
    "\n",
    "$$ \\frac{\\partial J(\\theta)}{\\partial \\theta} = X^T(S - Y) $$\n",
    "\n",
    "- $N$ is the number of data points\n",
    "- $Y_i$ is the true label (0 or 1) for the $i$th data point\n",
    "- $S_i$ is the predicted probability of the positive class for the $i$th data point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mllib LogisticRegression Accuracy: 0.860 \n",
      "Sklearn LogiscticRegression Accuracy: 0.847\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from mllib.logistic_regression import LogisticRegression as LogisticRegression_mllib\n",
    "from sklearn.linear_model import LogisticRegression as LogisticRegression_sklearn\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "lr_mllib = LogisticRegression_mllib().fit(X_train, y_train)\n",
    "y_pred = lr_mllib.predict(X_test)\n",
    "mllib_lr_acc =  accuracy_score(y_test, y_pred)\n",
    "\n",
    "lr_sklearn = LogisticRegression_sklearn().fit(X_train, y_train)\n",
    "y_pred = lr_sklearn.predict(X_test)\n",
    "sklearn_lr_acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Mllib LogisticRegression Accuracy: {mllib_lr_acc :5.3f} \\nSklearn LogiscticRegression Accuracy: {sklearn_lr_acc :5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression links:  \n",
    "\n",
    " - https://ai.plainenglish.io/l1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15  \n",
    " - https://ml-explained.com/blog/logistic-regression-explained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
