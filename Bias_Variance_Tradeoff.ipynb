{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Decomposition\n",
    "\n",
    "In machine learning, predictive models aim to minimize their error on unseen data. This error can be broken down into three components: **Bias**, **Variance**, and **Irreducible Error**. The bias-variance decomposition helps us understand how these components contribute to the overall error.\n",
    "\n",
    "Let's consider a regression problem where we aim to predict a target variable $y$ given features $x$. The true relationship between $X$ and $Y$ can be expressed as:\n",
    "\n",
    "$$y = f(x) + \\varepsilon$$\n",
    "\n",
    "Where:\n",
    "- $f(X)$ is the true (but unknown) function that maps features to the target.\n",
    "- $\\epsilon$ is random noise with zero mean and constant variance $\\sigma^2$.\n",
    "\n",
    "$\\mathbb{E} \\varepsilon = 0,  \\mathbb{V}\\text{ar} \\varepsilon = \\mathbb{E} \\varepsilon^2 = \\sigma^2.$  \n",
    "\n",
    "The goal of our model is to estimate $f(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Short Refresher:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{V}\\text{ar} [f(x)] &= \\mathbb{E}_x [f(x) - \\mathbb{E}_x[f(x)]]^2 \\\\\n",
    "    &= \\mathbb{E}_x [f^2(x) - 2f(x) \\mathbb{E}_x [f(x)] + \\mathbb{E}_x [f(x)]^2] \\\\\n",
    "    &= \\mathbb{E}_x [f^2(x)] - 2  \\mathbb{E}_x [f(x) \\mathbb{E}_x [f(x)]] +  \\mathbb{E}_x[\\mathbb{E}_x [f(x)]^2] \\\\\n",
    "    &= \\mathbb{E}_x [f^2(x)] - 2  \\mathbb{E}_x [f(x)] \\mathbb{E}_x [f(x)] + \\mathbb{E}_x [f(x)]^2 \\\\\n",
    "    &= \\mathbb{E}_x [f^2(x)] - 2  \\mathbb{E}_x [f(x)]^2 + \\mathbb{E}_x [f(x)]^2 \\\\\n",
    "    &= \\mathbb{E}_x [f^2(x)] -   \\mathbb{E}_x [f(x)]^2\\\\\n",
    "    \\\\\n",
    "\\text{Bias} [f(x)] &= f(x) - \\mathbb{E}_x[f(x)] \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_x[c] = c \\;\\text{  (c: constant)}\\\\\n",
    "&\\mathbb{E}_x[cx] = c\\mathbb{E}_x[x] \\;\\text{  (c: constant)}\\\\\n",
    "&\\mathbb{E}_x[\\mathbb{E}_x[x]] = \\mathbb{E}_x[x]\\\\\n",
    "&\\mathbb{E}_{x,y}[x + y] = \\mathbb{E}_{x,y}[x] + \\mathbb{E}_{x,y}[y]\\\\\n",
    "&\\mathbb{E}_{x,y}[xy] = \\mathbb{E}_{x}[x]\\mathbb{E}_{y}[y] \\;\\text{  (x,y: independent)}\\\\\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have subsample of the dataset $X$:  \n",
    "$$ X = ((x_1, y_1), \\ldots (x_l, y_l))$$ \n",
    "\n",
    "And the estimator of $y$ : $a(x)$, trained on this subsample $X$.\n",
    "$$a(x) = a(x, X)$$\n",
    "The goal of estimator is to correctly estimate the function $f(x)$, given the subsample $X$. Note that the target value $y$ is the function of $x \\text{ and } \\varepsilon$:  \n",
    "$$y(x) = y(x, \\varepsilon)$$\n",
    "\n",
    "The joined expectation $\\mathbb{E}_{X,\\varepsilon} = \\mathbb{E}_{X}\\mathbb{E}_{\\varepsilon}$ because $X$ and $\\varepsilon$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $x$ be test objects not present in training subsample $X$. The Mean Squared Error on test dataset will be:\n",
    "$$ Q(a) = \\mathbb{E}_x \\mathbb{E}_{X, \\varepsilon}\\left[y(x, \\varepsilon) - a(x, X)\\right]^2$$\n",
    "\n",
    "Now we expand the MSE expression and regroup it:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\mathbb{E}_{X, \\varepsilon}\\left[y(x, \\varepsilon) - a(x, X)\\right]^2 = \\\\\n",
    "&= \\mathbb{E}_{X, \\varepsilon}\\left[f(x) + \\varepsilon -a(x, X)\\right]^2 \\\\\n",
    "&= \\mathbb{E}_{X, \\varepsilon}\\left[(f(x) - a(x, X)) + \\varepsilon\\right]^2 \\\\\n",
    "&= \\mathbb{E}_{X, \\varepsilon}\\left[(f(x) - a(x, X))^2 + 2\\cdot\\varepsilon \\cdot (f(x) - a(x, X)) + \\varepsilon^2\\right] \\\\\n",
    "&= \\mathbb{E}_{X, \\varepsilon}\\left[(f(x) - a(x, X))^2\\right] + 2\\cdot \\mathbb{E}_{X, \\varepsilon}\\left[\\varepsilon \\cdot(f(x) - a(x, X))\\right] +  \\mathbb{E}_{X, \\varepsilon}\\left[\\varepsilon^2\\right] \\\\\n",
    "&= \\mathbb{E}_{X}\\left[(f(x) - a(x, X))^2\\right] + 2\\cdot\\mathbb{E}_{\\varepsilon}\\left[\\varepsilon\\right] \\cdot\\mathbb{E}_{X}\\left[(f(x) - a(x, X))\\right] +  \\mathbb{E}_{\\varepsilon}\\left[\\varepsilon^2\\right] \\\\\n",
    "&= \\mathbb{E}_{X}\\left[(f(x) - a(x, X))^2\\right] + 2 \\cdot 0 \\cdot\\mathbb{E}_{X}\\mathbb{E}_{\\varepsilon}\\left[(f(x) - a(x, X))\\right] +  \\sigma^2 \\\\\n",
    "&= \\mathbb{E}_{X}\\left[(f(x) - a(x, X))^2\\right] +  \\sigma^2 \\\\\n",
    "&= \\mathbb{E}_{X}\\left[(f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right] +\\mathbb{E}_{X}\\left[a(x, X)\\right] - a(x, X))^2\\right] +  \\sigma^2 \\\\\n",
    "&= \\mathbb{E}_{X}\\left[(f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right])^2\\right] +\\mathbb{E}_{X}\\left[( a(x, X)-\\mathbb{E}_{X}\\left[a(x, X)\\right])^2\\right] +2\\cdot\\mathbb{E}_{X}\\left[(f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right])(\\mathbb{E}_{X}\\left[a(x, X)\\right] - a(x, X))\\right] + \\sigma^2 \\\\\n",
    "&= (f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right])^2 + \\mathbb{V}\\text{ar}_X \\left[a(x, X) \\right] +2\\cdot(f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right])\\cdot \\mathbb{E}_{X}\\left[(\\mathbb{E}_{X}\\left[a(x, X)\\right] - a(x, X))\\right] + \\sigma^2 \\\\\n",
    "&= \\text{Bias}^2_X\\left[a(x, X)\\right] + \\mathbb{V}\\text{ar}_X \\left[a(x, X) \\right] +2\\cdot(f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right])\\cdot (\\mathbb{E}_{X}\\left[a(x, X)\\right] - \\mathbb{E}_{X}\\left[a(x, X)\\right]) + \\sigma^2 \\\\\n",
    "&= \\text{Bias}^2_X\\left[a(x, X)\\right] + \\mathbb{V}\\text{ar}_X \\left[a(x, X) \\right] +2\\cdot(f(x) -\\mathbb{E}_{X}\\left[a(x, X)\\right])\\cdot 0 + \\sigma^2 \\\\\n",
    "&= \\text{Bias}^2_X\\left[a(x, X)\\right] + \\mathbb{V}\\text{ar}_X \\left[a(x, X) \\right] + \\sigma^2 \\\\\n",
    "\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Decomposition and its Connection with Random Forest\n",
    "\n",
    "In machine learning, predictive models aim to minimize their error on unseen data. This error can be broken down into three components: **Bias**, **Variance**, and **Irreducible Error**. The bias-variance decomposition helps us understand how these components contribute to the overall error.\n",
    "\n",
    "Let's consider a regression problem where we aim to predict a target variable $Y$ given features $X$. The true relationship between $X$ and $Y$ can be expressed as:\n",
    "\n",
    "$$Y = f(X) + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- $f(X)$ is the true (but unknown) function that maps features to the target.\n",
    "- $\\epsilon$ is random noise with zero mean and constant variance $\\sigma^2$.\n",
    "\n",
    "The goal of our model is to estimate $f(X)$. In the context of Random Forest, we have an ensemble of decision trees. The bias-variance decomposition for Random Forest can be expressed as:\n",
    "\n",
    "$$ \\text{MSE} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error} $$\n",
    "\n",
    "Where:\n",
    "- **MSE (Mean Squared Error)** is the expected prediction error.\n",
    "- **Bias** measures the error due to overly simplistic assumptions in the model. It quantifies how much our model's predictions differ from the true function $f(X)$.\n",
    "- **Variance** measures the error due to model instability. It quantifies how much the predictions for $Y$ vary as we fit the model to different training datasets.\n",
    "- **Irreducible Error** represents the noise inherent in the data, which cannot be reduced no matter how complex the model is.\n",
    "\n",
    "Now, let's derive the expressions for Bias and Variance:\n",
    "\n",
    "### Bias:\n",
    "Bias can be defined as the expected difference between our model's predictions and the true function $f(X)$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Bias} &= \\mathbb{E}[(\\hat{f}(X) - f(X))^2] \\\\\n",
    "&= \\mathbb{E}[\\hat{f}(X)^2] - 2\\mathbb{E}[\\hat{f}(X)f(X)] + \\mathbb{E}[f(X)^2]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In Random Forest, each tree provides an estimate $\\hat{f}_i(X)$. Assuming the trees are uncorrelated, the expected value of their squared predictions is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{f}(X)^2] = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbb{E}[\\hat{f}_i(X)^2]\n",
    "$$\n",
    "\n",
    "Where $N$ is the number of trees. Now, let's calculate the second term:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\hat{f}(X)f(X)] &= \\frac{1}{N}\\sum_{i=1}^{N} \\mathbb{E}[\\hat{f}_i(X)f(X)] \\\\\n",
    "&= \\frac{1}{N}\\sum_{i=1}^{N} \\mathbb{E}[\\hat{f}_i(X)]\\mathbb{E}[f(X)]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since Random Forest averages the predictions of its trees, $\\mathbb{E}[\\hat{f}_i(X)] = \\mathbb{E}[f(X)]$ for each tree.\n",
    "\n",
    "Finally, the Bias term simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Bias} = \\frac{1}{N}\\sum_{i=1}^{N} \\mathbb{E}[\\hat{f}_i(X)^2] - \\mathbb{E}[f(X)^2]\n",
    "$$\n",
    "\n",
    "### Variance:\n",
    "Variance measures how much the predictions of our model vary across different training datasets. For Random Forest, this can be expressed as:\n",
    "\n",
    "$$\n",
    "\\text{Variance} = \\frac{1}{N}\\sum_{i=1}^{N} \\text{Var}(\\hat{f}_i(X))\n",
    "$$\n",
    "\n",
    "Where $\\text{Var}(\\hat{f}_i(X))$ is the variance of the predictions of the $i$-th tree.\n",
    "\n",
    "In summary, the bias-variance decomposition helps us understand the trade-off between model complexity (which affects variance) and model simplicity (which affects bias). Random Forest mitigates overfitting by aggregating the predictions of multiple trees, effectively reducing variance while maintaining low bias.\n",
    "\n",
    "Understanding this decomposition is essential for model selection and tuning to achieve optimal predictive performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
