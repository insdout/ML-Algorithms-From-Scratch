{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and its variants.\n",
    "\n",
    "Gradient Descent (GD) is a fundamental optimization algorithm widely used in machine learning and mathematical optimization. It serves as the backbone for training various models, including linear regression, neural networks, and support vector machines. The main objective of Gradient Descent is to find the optimal parameters of a model that minimize or maximize a given objective function.\n",
    "\n",
    "In the context of supervised learning, the objective function typically represents the loss or cost function, which measures the discrepancy between the model's predictions and the actual ground-truth labels. The goal of Gradient Descent is to iteratively update the model parameters in a way that reduces this discrepancy, effectively improving the model's performance.\n",
    "\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Let's define the following terms for the mathematical formulation of GD:\n",
    "\n",
    "- $\\theta$: Model parameters that we want to optimize.\n",
    "- $J(\\theta)$: The loss function that measures the difference between predicted and true labels.\n",
    "- $\\eta$: Learning rate, which controls the step size of parameter updates.\n",
    "- $X$: The input data.\n",
    "- $y$: The true labels corresponding to the input data. \n",
    "- $f_\\theta$: parametric function, $\\hat{y} = f_\\theta(x)$ \n",
    "\n",
    "We will define the loss function as follows:  \n",
    "$J(\\theta) = \\sum_{n=1}^{N}\\mathcal{L}(y_n, f_{\\theta}(x_n))$  \n",
    "\n",
    "The objective of GD is to find the optimal parameters $\\theta^*$ that minimize the loss function $J(\\theta)$:  \n",
    "$\\theta^* = \\arg \\min_{\\theta} J(\\theta)$\n",
    "\n",
    "\n",
    "The update rule for GD can be expressed as:\n",
    "\n",
    "$\\theta_{t+1} = \\theta_{t} - \\eta \\cdot \\nabla J(\\theta_{t})$\n",
    "\n",
    "where $\\nabla J(\\theta)$ represents the gradient of the loss function with respect to the model parameters $\\theta$.\n",
    "\n",
    "## Variants of GD\n",
    "\n",
    "GD has several variants that incorporate additional techniques to improve convergence and stability. Some of the popular variants include:\n",
    "\n",
    "1. **Mini-batch SGD**: Instead of using a all data points, it uses a small mini-batch of data points to compute the gradient at each iteration. This strikes a balance between efficiency and stability.\n",
    "\n",
    "2. **Momentum**: Momentum introduces a momentum term that helps the optimization process to overcome oscillations and accelerate convergence. It accumulates a weighted average of past gradients to update the parameters.\n",
    "\n",
    "3. **Nesterov Momentum**: Nesterov Momentum is an extension of classic Momentum that takes into account the lookahead position when computing the gradient, leading to faster convergence.\n",
    "\n",
    "4. **RMSprop**: RMSprop adapts the learning rate for each parameter based on the root mean square of the past gradients. It helps to handle different scales of gradients and improves convergence.\n",
    "\n",
    "5. **Adam**: Adam combines the ideas of Momentum and RMSprop. It adapts the learning rate for each parameter based on the first and second moments of the past gradients.\n",
    "\n",
    "Each variant of GD has its strengths and is suited for different scenarios. The choice of the variant often depends on the specific problem and the characteristics of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch SGD\n",
    "\n",
    "Let us assume that the dataset is split in M minibatches each of size M. Than we will obtain following sequences of data:  \n",
    "$\\underbrace{a=b}_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Momentum\n",
    "\n",
    "Nesterov Momentum is an extension of the classic Momentum optimization algorithm. It addresses the issue of oscillations in the optimization process, allowing for faster convergence. In Nesterov Momentum, the algorithm considers the lookahead position when computing the gradient, which helps to better estimate the direction of the gradient and adjust the velocity accordingly.\n",
    "\n",
    "The update rule for Nesterov Momentum is as follows:\n",
    "\n",
    "1. Calculate the lookahead position $\\theta_{\\text{lookahead}}$:\n",
    "   $$\n",
    "   \\theta_{\\text{lookahead}} = \\theta + \\mu \\cdot \\text{velocity}\n",
    "   $$\n",
    "\n",
    "2. Compute the gradient of the loss function with respect to the lookahead position $\\theta_{\\text{lookahead}}$:\n",
    "   $$\n",
    "   \\nabla J(\\theta_{\\text{lookahead}})\n",
    "   $$\n",
    "\n",
    "3. Update the velocity using the previous velocity, learning rate $\\eta$, and the gradient at the lookahead position:\n",
    "   $$\n",
    "   \\text{velocity} = \\mu \\cdot \\text{velocity} - \\eta \\cdot \\nabla J(\\theta_{\\text{lookahead}})\n",
    "   $$\n",
    "\n",
    "4. Finally, update the model parameters using the velocity and the learning rate:\n",
    "   $$\n",
    "   \\theta = \\theta + \\text{velocity}\n",
    "   $$\n",
    "\n",
    "Nesterov Momentum effectively reduces oscillations and improves the convergence speed compared to classic Momentum, making it a powerful optimization technique.\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm. It helps to handle the issues of varying gradient scales and ill-conditioned optimization surfaces. RMSProp adaptively adjusts the learning rate for each parameter based on the root mean square of the past gradients.\n",
    "\n",
    "The update rule for RMSProp is as follows:\n",
    "\n",
    "1. Initialize a moving average of the squared gradients:\n",
    "   $$\n",
    "   \\text{squared\\_gradients} = 0\n",
    "   $$\n",
    "\n",
    "2. Compute the squared gradients for each parameter using the decay rate $\\gamma$:\n",
    "   $$\n",
    "   \\text{squared\\_gradients} = \\gamma \\cdot \\text{squared\\_gradients} + (1 - \\gamma) \\cdot (\\nabla J(\\theta))^2\n",
    "   $$\n",
    "\n",
    "3. Update the model parameters using the learning rate $\\eta$ and the scaled gradient:\n",
    "   $$\n",
    "   \\theta = \\theta - \\frac{\\eta}{\\sqrt{\\text{squared\\_gradients} + \\epsilon}} \\cdot \\nabla J(\\theta)\n",
    "   $$\n",
    "\n",
    "RMSProp adapts the learning rate for each parameter based on the history of gradients, allowing for faster convergence and better optimization performance.\n",
    "\n",
    "## Adam\n",
    "\n",
    "Adam (Adaptive Moment Estimation) is another adaptive learning rate optimization algorithm that combines the ideas of Momentum and RMSProp. It not only adapts the learning rate for each parameter but also keeps track of the first and second moments of the past gradients.\n",
    "\n",
    "The update rule for Adam is as follows:\n",
    "\n",
    "1. Initialize the first and second moment estimates:\n",
    "   $$\n",
    "   \\text{m} = 0, \\quad \\text{v} = 0\n",
    "   $$\n",
    "\n",
    "2. Update the first moment estimate:\n",
    "   $$\n",
    "   \\text{m} = \\beta_1 \\cdot \\text{m} + (1 - \\beta_1) \\cdot \\nabla J(\\theta)\n",
    "   $$\n",
    "\n",
    "3. Update the second moment estimate:\n",
    "   $$\n",
    "   \\text{v} = \\beta_2 \\cdot \\text{v} + (1 - \\beta_2) \\cdot (\\nabla J(\\theta))^2\n",
    "   $$\n",
    "\n",
    "4. Compute bias-corrected first and second moment estimates:\n",
    "   $$\n",
    "   \\hat{\\text{m}} = \\frac{\\text{m}}{1 - \\beta_1^t}, \\quad \\hat{\\text{v}} = \\frac{\\text{v}}{1 - \\beta_2^t}\n",
    "   $$\n",
    "\n",
    "5. Update the model parameters using the learning rate $\\eta$ and the bias-corrected estimates:\n",
    "   $$\n",
    "   \\theta = \\theta - \\frac{\\eta}{\\sqrt{\\hat{\\text{v}}} + \\epsilon} \\cdot \\hat{\\text{m}}\n",
    "   $$\n",
    "\n",
    "Adam efficiently combines the benefits of both Momentum and RMSProp, making it a popular choice for optimizing deep learning models.\n",
    "\n",
    "These optimization algorithms offer various improvements over traditional Gradient Descent and play a crucial role in training complex machine learning models effectively and efficiently. The choice of the optimization algorithm depends on the specific problem, dataset, and model architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mllib.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_opt: [3.00002925] f_opy: [8.55525428e-10] grad_fn: [5.84986886e-05]\n"
     ]
    }
   ],
   "source": [
    "from mllib.optimizers import Adam\n",
    "import numpy as np \n",
    "\n",
    "def loss_fn(z, y, x):\n",
    "        return x**2 - 6*x + 9\n",
    "\n",
    "def gradient_fn(z, y, x):\n",
    "    return 2*x - 6\n",
    "\n",
    "adam = Adam(\n",
    "        gradient_fn=gradient_fn,\n",
    "        parameters=np.array([20.]),\n",
    "        loss_fn=loss_fn,\n",
    "        learning_rate=1e-1,\n",
    "        max_iter=1000,\n",
    "        tolerance=1e-12,\n",
    "        batch_size=1\n",
    "        )\n",
    "X = y = np.array([0])\n",
    "x_opt = adam.optimize(X, y)\n",
    "assert abs(x_opt-3) <= 1e2, \"Wrong answer\"\n",
    "print(f\"x_opt: {x_opt} f_opy: {loss_fn(None, None, x_opt)} \"\n",
    "        f\"grad_fn: {gradient_fn(None, None, x_opt)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
