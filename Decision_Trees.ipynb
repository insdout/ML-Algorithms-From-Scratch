{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision trees are versatile and interpretable machine learning models used for both classification and regression tasks. They work by recursively partitioning the input space into regions, assigning a label or value to each region based on the majority class or average target value of the training data within that region. In this section, we'll explore the mathematical formulation of decision trees and delve into two commonly used criteria for splitting nodes: entropy and Gini impurity.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "A decision tree can be represented as a binary tree, where each internal node represents a decision or test on a feature, each branch corresponds to a possible outcome of the test, and each leaf node holds a class label (in classification) or a predicted value (in regression). Let's denote the decision tree as $T$.\n",
    "\n",
    "### Entropy Criterion\n",
    "\n",
    "Entropy is a measure of impurity in a set of samples. For a classification problem with $K$ distinct classes, the entropy of a set $S$ with respect to class distribution $p_1, p_2, \\ldots, p_K$ is defined as:\n",
    "\n",
    "$$\n",
    "H(S) = -\\sum_{i=1}^{K} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "where $p_i$ is the proportion of samples in class $i$ within set $S$.\n",
    "\n",
    "The information gain (IG) is used to evaluate the quality of a split based on entropy. Given a set $S$ of samples, and $V$ a split of $S$ into subsets $S_1, S_2, \\ldots, S_V$, the information gain is defined as:\n",
    "\n",
    "$$\n",
    "IG(S, V) = H(S) - \\sum_{v=1}^{V} \\frac{|S_v|}{|S|} H(S_v)\n",
    "$$\n",
    "\n",
    "The decision tree algorithm aims to maximize the information gain when choosing the best split at each node.\n",
    "\n",
    "### Gini Impurity Criterion\n",
    "\n",
    "Gini impurity is another measure of impurity. For a classification problem with $K$ classes, the Gini impurity of a set $S$ is defined as:\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - \\sum_{i=1}^{K} p_i^2\n",
    "$$\n",
    "\n",
    "where $p_i$ is the proportion of samples in class $i$ within set $S$.\n",
    "\n",
    "Similar to entropy, the Gini impurity can be used to compute the impurity of a split. Given a set $S$ of samples and a split $V$ into subsets $S_1, S_2, \\ldots, S_V$, the Gini impurity of the split is defined as:\n",
    "\n",
    "$$\n",
    "Gini(S, V) = \\sum_{v=1}^{V} \\frac{|S_v|}{|S|} Gini(S_v)\n",
    "$$\n",
    "\n",
    "The decision tree algorithm seeks to minimize the Gini impurity when selecting the best split.\n",
    "\n",
    "Both entropy and Gini impurity criteria are widely used in decision tree algorithms, such as CART (Classification and Regression Trees) and C4.5. These criteria guide the construction of the tree by iteratively selecting the best feature and split that maximizes information gain or minimizes impurity.\n",
    "\n",
    "Decision trees provide interpretable models and are a fundamental building block for ensemble methods like random forests and gradient boosting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Gini Impurity\n",
    "\n",
    "The Gini impurity is a measure of impurity or disorder used in decision trees for classification problems. It represents the probability of misclassifying a randomly selected sample from a set. Let's derive the formula for Gini impurity step by step.\n",
    "\n",
    "### Step 1: Notation\n",
    "\n",
    "For a classification problem with $K$ distinct classes, let $p_1, p_2, \\ldots, p_K$ be the proportions of samples in each class within a set $S$. We denote $p_i$ as the proportion of samples in class $i$ within set $S$.\n",
    "\n",
    "### Step 2: Define Gini Impurity\n",
    "\n",
    "The Gini impurity for a set $S$, denoted as $Gini(S)$, is defined as:\n",
    "\n",
    "$$\n",
    "Gini(S) = 1 - \\sum_{i=1}^{K} p_i^2\n",
    "$$\n",
    "\n",
    "### Step 3: Interpretation\n",
    "\n",
    "The Gini impurity represents the probability of misclassifying a randomly selected sample from set $S$. Here's the derivation:\n",
    "\n",
    "- $1$ is the total probability of selecting a sample from the set.\n",
    "- $\\sum_{i=1}^{K} p_i^2$ is the probability that a randomly selected sample will be correctly classified. This is calculated as the sum of the squared proportions of each class within the set.\n",
    "\n",
    "### Step 4: Derivation\n",
    "\n",
    "Let's derive $Gini(S)$ explicitly:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Gini(S) &= 1 - \\sum_{i=1}^{K} p_i^2 \\\\\n",
    "&= 1 - (p_1^2 + p_2^2 + \\ldots + p_K^2) \\\\\n",
    "&= 1 - p_1^2 - p_2^2 - \\ldots - p_K^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Step 5: Interpretation (Revisited)\n",
    "\n",
    "- $1$ is the total probability of selecting a sample.\n",
    "- $p_1^2$ is the probability of correctly classifying a sample as class $1$.\n",
    "- $p_2^2$ is the probability of correctly classifying a sample as class $2$, and so on.\n",
    "- The subtraction from $1$ represents the probability of misclassification.\n",
    "\n",
    "### Step 6: Conclusion\n",
    "\n",
    "The Gini impurity ($Gini(S)$) quantifies the impurity or disorder in a set $S$ by measuring the probability of misclassification when randomly selecting a sample from that set. A lower Gini impurity indicates a purer set with less impurity, while a higher Gini impurity implies more mixing of classes within the set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Entropy Criterion\n",
    "\n",
    "The Entropy criterion is used in decision trees for classification problems. It measures the impurity or disorder within a set and quantifies the uncertainty associated with class labels. Let's derive the formula for Entropy step by step.\n",
    "\n",
    "### Step 1: Notation\n",
    "\n",
    "For a classification problem with $K$ distinct classes, let $p_1, p_2, \\ldots, p_K$ be the proportions of samples in each class within a set $S$. We denote $p_i$ as the proportion of samples in class $i$ within set $S$.\n",
    "\n",
    "### Step 2: Define Entropy\n",
    "\n",
    "The Entropy for a set $S$, denoted as $Entropy(S)$, is defined as:\n",
    "\n",
    "$$\n",
    "Entropy(S) = - \\sum_{i=1}^{K} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "### Step 3: Interpretation\n",
    "\n",
    "The Entropy criterion measures the average amount of information needed to predict the class of a randomly selected sample from set $S$. Here's the derivation:\n",
    "\n",
    "- The negative sign in front of the summation ensures that Entropy is a positive value representing uncertainty.\n",
    "- $\\sum_{i=1}^{K} p_i \\log_2(p_i)$ is the information content associated with class probabilities. This is calculated as the sum of the products of class proportions ($p_i$) and their logarithms.\n",
    "\n",
    "### Step 4: Derivation\n",
    "\n",
    "Let's derive $Entropy(S)$ explicitly:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "Entropy(S) &= - \\sum_{i=1}^{K} p_i \\log_2(p_i) \\\\\n",
    "&= - (p_1 \\log_2(p_1) + p_2 \\log_2(p_2) + \\ldots + p_K \\log_2(p_K))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Step 5: Interpretation (Revisited)\n",
    "\n",
    "- The negative sign indicates that the more certain or pure a set is, the smaller its entropy.\n",
    "- $p_1 \\log_2(p_1)$ is the information content associated with correctly classifying a sample as class $1$.\n",
    "- $p_2 \\log_2(p_2)$ is the information content for correctly classifying as class $2$, and so on.\n",
    "- The summation represents the average information content across all possible outcomes.\n",
    "\n",
    "### Step 6: Conclusion\n",
    "\n",
    "The Entropy criterion ($Entropy(S)$) quantifies the impurity or disorder in a set $S$ by measuring the average information needed to predict the class of a randomly selected sample from that set. Lower entropy indicates a purer set with less impurity, while higher entropy implies more mixing of classes within the set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation of Entropy Criterion from Information Gain\n",
    "\n",
    "The Entropy criterion is used in decision trees for classification problems to measure impurity or disorder within a set. It can be derived from the concept of Information Gain, which quantifies the reduction in uncertainty about a class label when a set is split based on a specific attribute. Let's derive the Entropy criterion step by step.\n",
    "\n",
    "### Step 1: Notation\n",
    "\n",
    "For a classification problem with $K$ distinct classes, let $p_1, p_2, \\ldots, p_K$ be the proportions of samples in each class within a set $S$. We denote $p_i$ as the proportion of samples in class $i$ within set $S$.\n",
    "\n",
    "### Step 2: Define Information Gain\n",
    "\n",
    "The Information Gain (IG) when splitting set $S$ into subsets $S_1, S_2, \\ldots, S_m$ based on an attribute is defined as:\n",
    "\n",
    "$$\n",
    "IG(S, A) = H(S) - \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} \\cdot H(S_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $H(S)$ is the entropy of set $S$.\n",
    "- $H(S_i)$ is the entropy of subset $S_i$.\n",
    "- $|S|$ is the total number of samples in set $S$.\n",
    "- $|S_i|$ is the number of samples in subset $S_i$.\n",
    "\n",
    "### Step 3: Define Entropy\n",
    "\n",
    "The Entropy for a set $S$, denoted as $Entropy(S)$, is defined as:\n",
    "\n",
    "$$\n",
    "Entropy(S) = - \\sum_{i=1}^{K} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "### Step 4: Derivation of Information Gain\n",
    "\n",
    "Let's derive $IG(S, A)$ using the definition of entropy:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "IG(S, A) &= H(S) - \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} \\cdot H(S_i) \\\\\n",
    "&= -\\sum_{i=1}^{K} p_i \\log_2(p_i) - \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} \\left(-\\sum_{j=1}^{K_i} p_{ij} \\log_2(p_{ij})\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $K$ is the total number of classes.\n",
    "- $K_i$ is the number of classes in subset $S_i$.\n",
    "- $p_i$ is the proportion of samples in class $i$ within set $S$.\n",
    "- $p_{ij}$ is the proportion of samples in class $j$ within subset $S_i$.\n",
    "\n",
    "### Step 5: Further Derivation\n",
    "\n",
    "Simplify the expression:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "IG(S, A) &= -\\sum_{i=1}^{K} p_i \\log_2(p_i) + \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} \\sum_{j=1}^{K_i} p_{ij} \\log_2(p_{ij}) \\\\\n",
    "&= -\\sum_{i=1}^{K} p_i \\log_2(p_i) + \\sum_{i=1}^{m} \\frac{|S_i|}{|S|} \\left(-\\sum_{j=1}^{K_i} \\frac{|S_i|}{|S|} p_{ij} \\log_2(p_{ij})\\right) \\\\\n",
    "&= -\\sum_{i=1}^{K} p_i \\log_2(p_i) + \\sum_{i=1}^{m} \\left(-\\frac{|S_i|}{|S|}\\right) \\sum_{j=1}^{K_i} \\left(\\frac{|S_i|}{|S|}\\right) p_{ij} \\log_2(p_{ij})\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Step 6: Interpreting Information Gain\n",
    "\n",
    "The Information Gain $IG(S, A)$ quantifies the reduction in uncertainty about the class labels achieved by splitting set $S$ based on attribute $A$. A higher $IG$ indicates a better attribute for the split, as it reduces the entropy (uncertainty) within subsets $S_1, S_2, \\ldots, S_m$.\n",
    "\n",
    "### Step 7: Entropy Criterion\n",
    "\n",
    "Now, the Entropy criterion for decision tree splitting can be defined as:\n",
    "\n",
    "$$\n",
    "EntropyCriterion(S, A) = - \\sum_{i=1}^{K} p_i \\log_2(p_i) - IG(S, A)\n",
    "$$\n",
    "\n",
    "Where $A$ is the attribute being considered for the split.\n",
    "\n",
    "The\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini_inpurity(y):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    probabilities = np.bincount(y)/y.shape[0]\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    print(f\"probabilities squared: {probabilities**2}\")\n",
    "    print(f\"gini inpurity: {1 - np.sum(probabilities**2)}\")\n",
    "    return 1 - np.sum(probabilities**2)\n",
    "\n",
    "def entropy_inpurity(y):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    probabilities = np.bincount(y)/y.shape[0]\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    print(f\"log probabilities: { np.log2(probabilities, where=(probabilities > 0))}\")\n",
    "    print(f\"p*log_2 p: { probabilities*np.log2(probabilities, where=(probabilities > 0))}\")\n",
    "    print(f\"entropy inpurity: {np.sum(-probabilities * np.log2(probabilities, where=(probabilities > 0)))}\")\n",
    "    return np.sum(-probabilities * np.log2(probabilities, where=(probabilities > 0)))\n",
    "\n",
    "y = [0, 0, 1, 1]\n",
    "gini_inpurity(y)\n",
    "print(\"===================\")\n",
    "entropy_inpurity(y)\n",
    "print()\n",
    "y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "gini_inpurity(y)\n",
    "print(\"===================\")\n",
    "entropy_inpurity(y)\n",
    "\n",
    "print()\n",
    "y = [1, 1, 1, 1, 1, 1, 1]\n",
    "gini_inpurity(y)\n",
    "print(\"===================\")\n",
    "entropy_inpurity(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine an experiment with $k$ possible output categories. Category $j$ has a probability of occurrence $p(j|t)$ (where $j=1,\\ldots,k$).\n",
    "\n",
    "Reproduce the experiment two times and make these observations:\n",
    "\n",
    "1. The probability of obtaining two identical outputs of category $j$ is $p^2(j|t)$.\n",
    "2. The probability of obtaining two identical outputs, independently of their category, is: $\\sum_{j=1}^{k} p^2(j|t)$.\n",
    "3. The probability of obtaining two different outputs is thus: $1 - \\sum_{j=1}^{k} p^2(j|t)$.\n",
    "\n",
    "That's it: the Gini impurity is simply the probability of obtaining two different outputs, which is an \"impurity measure\".\n",
    "\n",
    "https://stats.stackexchange.com/questions/308885/a-simple-clear-explanation-of-the-gini-impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine an experiment with $k$ possible output categories. Category $j$ has a probability of occurrence $p(j|t)$ (where $j=1,\\ldots,k$).\n",
    "\n",
    "Reproduce the experiment two times and make these observations:\n",
    "\n",
    "1. The probability of obtaining two identical outputs of category $j$ is $p^2(j|t)$.\n",
    "2. The probability of obtaining two identical outputs, independently of their category, is: $\\sum_{j=1}^{k} p^2(j|t)$.\n",
    "3. The probability of obtaining two different outputs is thus: $1 - \\sum_{j=1}^{k} p^2(j|t)$.\n",
    "\n",
    "That's it: the Gini impurity is simply the probability of obtaining two different outputs, which is an \"impurity measure\".\n",
    "\n",
    "https://stats.stackexchange.com/questions/308885/a-simple-clear-explanation-of-the-gini-impurity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' 'good']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'good'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mprint\u001b[39m(X[\u001b[39m0\u001b[39m,:])\n\u001b[1;32m     17\u001b[0m clf \u001b[39m=\u001b[39m DecisionTreeClassifier(max_leaf_nodes\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_dl/lib/python3.8/site-packages/sklearn/tree/_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    860\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \n\u001b[1;32m    862\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    890\u001b[0m         X,\n\u001b[1;32m    891\u001b[0m         y,\n\u001b[1;32m    892\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    893\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_dl/lib/python3.8/site-packages/sklearn/tree/_classes.py:186\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    184\u001b[0m check_X_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(dtype\u001b[39m=\u001b[39mDTYPE, accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsc\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    185\u001b[0m check_y_params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 186\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    187\u001b[0m     X, y, validate_separately\u001b[39m=\u001b[39;49m(check_X_params, check_y_params)\n\u001b[1;32m    188\u001b[0m )\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m issparse(X):\n\u001b[1;32m    190\u001b[0m     X\u001b[39m.\u001b[39msort_indices()\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_dl/lib/python3.8/site-packages/sklearn/base.py:560\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_X_params:\n\u001b[1;32m    559\u001b[0m     check_X_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_X_params}\n\u001b[0;32m--> 560\u001b[0m X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_X_params)\n\u001b[1;32m    561\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mestimator\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m check_y_params:\n\u001b[1;32m    562\u001b[0m     check_y_params \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_check_params, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params}\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_dl/lib/python3.8/site-packages/sklearn/utils/validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    877\u001b[0m         array \u001b[39m=\u001b[39m xp\u001b[39m.\u001b[39mastype(array, dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    878\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 879\u001b[0m         array \u001b[39m=\u001b[39m _asarray_with_order(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype, xp\u001b[39m=\u001b[39;49mxp)\n\u001b[1;32m    880\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    881\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    882\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    883\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml_dl/lib/python3.8/site-packages/sklearn/utils/_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[1;32m    182\u001b[0m     xp, _ \u001b[39m=\u001b[39m get_namespace(array)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m xp\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mnumpy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnumpy.array_api\u001b[39m\u001b[39m\"\u001b[39m}:\n\u001b[1;32m    184\u001b[0m     \u001b[39m# Use NumPy API to support order\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    186\u001b[0m     \u001b[39mreturn\u001b[39;00m xp\u001b[39m.\u001b[39masarray(array, copy\u001b[39m=\u001b[39mcopy)\n\u001b[1;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'good'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X = np.array([[1,2, \"good\"], [4,5,\"bad\"]])\n",
    "y = np.array([1, 0])\n",
    "print(X[0,:])\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(max_leaf_nodes=3, random_state=0)\n",
    "clf.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
