{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a foundational technique in statistics and machine learning that models the relationship between a dependent variable and one or more independent variables. It aims to find the best-fitting linear equation that predicts the dependent variable based on the independent variables. In this chapter, we will delve into the concepts of linear regression, its assumptions, mathematical representation, and different solution methods.\n",
    "\n",
    "## Assumptions of Linear Regression\n",
    "\n",
    "Before applying linear regression, it's important to consider its underlying assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between independent variables and the dependent variable is assumed to be linear.\n",
    "\n",
    "2. **Independence**: The residuals (differences between actual and predicted values) are independent of each other.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals is constant across all levels of the independent variables.\n",
    "\n",
    "4. **Normality**: The residuals follow a normal distribution.\n",
    "\n",
    "5. **No Multicollinearity**: The independent variables are not highly correlated with each other.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Let's define the following terms for the mathematical formulation of linear regression:\n",
    "\n",
    "- **X**: The matrix of independent variables (features), with an added column of ones for the intercept term.\n",
    "- **y**: The vector of target values.\n",
    "- **θ**: The vector of coefficients (weights) that we want to optimize.\n",
    "- **n**: The number of data points.\n",
    "- **m**: The number of features.\n",
    "\n",
    "The extended linear regression equation can be written as:\n",
    "\n",
    "$$\n",
    "y = Xθ + ε\n",
    "$$\n",
    "\n",
    "where ε represents the error term.\n",
    "\n",
    "## Direct Solution: Normal Equation\n",
    "\n",
    "One way to find the optimal coefficients θ directly is by using the normal equation. The normal equation provides an analytical solution to the least squares problem, minimizing the sum of squared errors.\n",
    "\n",
    "The normal equation is:\n",
    "\n",
    "$$\n",
    "θ = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X^T}$: The transpose of the feature matrix X.\n",
    "- $\\mathbf{y}$: The vector of target values.\n",
    "\n",
    "The normal equation directly calculates the optimal values of θ that minimize the loss function.\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The goal of linear regression is to find the coefficients θ that minimize the difference between the predicted values (Xθ) and the actual target values (y). The most common loss function used is the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "J(θ) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i -  \\sum_{j=1}^{k}X_{i,j}θ_j)^2\n",
    "$$\n",
    "\n",
    "where n is the number of data points, and $X_i$ represents the i-th row of the feature matrix X.\n",
    "\n",
    "## Gradient Descent for Linear Regression\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize the loss function. The gradient of the loss function with respect to the coefficients θ is computed, and θ is updated in the opposite direction of the gradient. The update rule is:\n",
    "\n",
    "$$\n",
    "θ := θ - α \\nabla J(θ)\n",
    "$$\n",
    "\n",
    "where α is the learning rate.\n",
    "\n",
    "The gradient of the loss function can be computed as:\n",
    "\n",
    "$$\n",
    "\\nabla J(θ) =  X^T(Xθ - y)\n",
    "$$\n",
    "\n",
    "\n",
    "Linear regression serves as a powerful tool for modeling relationships between variables. By considering its assumptions and understanding various solution methods, such as gradient descent and the normal equation, we can apply linear regression effectively to various prediction tasks. Its simplicity, interpretability, and versatility make linear regression a cornerstone of both statistical analysis and machine learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression\n",
    "\n",
    "Before applying linear regression, it's important to consider its underlying assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between independent variables and the dependent variable is assumed to be linear.  \n",
    "\n",
    "2. **Independence**: The residuals (differences between actual and predicted values) are independent of each other.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals is constant across all levels of the independent variables.\n",
    "\n",
    "4. **Normality**: The residuals follow a normal distribution.\n",
    "\n",
    "5. **No Multicollinearity**: The independent variables are not highly correlated with each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity in the context of linear regression refers to a situation where two or more predictor variables (features) in your dataset are highly correlated. This high correlation can lead to instability and issues when computing the direct solution of linear regression, $y = X\\theta$, where y is the target variable, $X$ is the design matrix of predictor variables, and $\\theta$ is the vector of coefficients that we want to estimate.\n",
    "\n",
    "When multicollinearity is present, it means that some predictor variables can be approximately expressed as a linear combination of other predictor variables. This can cause problems when trying to compute the pseudo-inverse of the matrix $X$, which is needed to calculate the least squares estimates of the coefficients $\\theta$.\n",
    "\n",
    "**Computation of pseudo-inverse of $X$**:\n",
    "- Direct computation:  \n",
    "\n",
    "    $X^+ = (X^TX)^{-1}X^T$\n",
    "\n",
    "- via SVD:\n",
    "$$ \n",
    "\\begin{gather*}\n",
    "X = U\\Sigma V^T \\\\\n",
    "X^+ = V \\Sigma^{-1} U^T\n",
    "\\end{gather*}\n",
    "$$\n",
    "Where $\\Sigma  = diag(\\sigma_1(X), \\sigma_2(X), \\cdots, \\sigma_n(X)) = diag(\\sqrt{\\lambda_1(X^TX)}, \\sqrt{\\lambda_2(X^TX)}, \\cdots, \\sqrt{\\lambda_n(X^TX)})$\n",
    "\n",
    "Since $rank(X^TX) = rank(X)$, if $rank(X_{m \\times n}) < \\min(m, n)$ than $X^TX$ is singular.  It means that some of its eigenvalues are zero or very close to zero.\n",
    "\n",
    "When performing matrix operations involving a singular or ill-conditioned matrix, numerical instability can occur. Small numerical errors can lead to large errors in the computed solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to detect multicollinearity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Condition number\n",
    "\n",
    "Ill-conditioning occurs when the condition number of the matrix $X^T X$ is very large. The condition number is a measure of how sensitive the solution is to changes in the input data. A high condition number means that small changes in the input can lead to large changes in the output, making the solution unstable.\n",
    "\n",
    "#### Lets numerically evaluate the stability of the solution of $y = X\\theta$:\n",
    "\n",
    "We define:\n",
    "- $\\delta \\boldsymbol{\\theta}$: as perturbation of $\\boldsymbol{\\theta}$\n",
    "- $\\delta \\boldsymbol{y}$: as perturbation of $y$\n",
    "- $\\delta \\boldsymbol{X}$: as perturbation of $\\boldsymbol{X}$\n",
    "- $||\\boldsymbol{X}||$: as second matrix norm, $||\\boldsymbol{X}||_2 = \\max \\boldsymbol{\\sigma}(\\boldsymbol{X})$\n",
    "- $\\boldsymbol{\\sigma}(\\boldsymbol{X})$: singular values of matrix $\\boldsymbol{X}$\n",
    "\n",
    "##### 1. How changes in $y$ impact $\\theta$:\n",
    "\n",
    "$$ \\boldsymbol{X}(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta}) = \\boldsymbol{y} + \\delta \\boldsymbol{y} $$\n",
    "\n",
    "Since $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}$:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\boldsymbol{X}\\boldsymbol{\\theta} + \\boldsymbol{X}\\delta \\boldsymbol{\\theta} = \\boldsymbol{y} + \\delta \\boldsymbol{y} \\\\\n",
    "\\boldsymbol{y} + \\boldsymbol{X}\\delta \\boldsymbol{\\theta} = \\boldsymbol{y} + \\delta \\boldsymbol{y} \\\\\n",
    "\\boldsymbol{X}\\delta \\boldsymbol{\\theta} = \\delta \\boldsymbol{y} \\\\\n",
    "\\delta \\boldsymbol{\\theta} = \\boldsymbol{X}^{-1} \\delta \\boldsymbol{y} \\\\\n",
    "||\\delta \\boldsymbol{\\theta}|| = ||\\boldsymbol{X}^{-1} \\delta \\boldsymbol{y}|| \\\\\n",
    "||\\delta \\boldsymbol{\\theta}|| \\leq ||\\boldsymbol{X}^{-1}||\\cdot||\\delta \\boldsymbol{y}|| \\\\\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "From $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}$ we get:\n",
    "\n",
    "$$||\\boldsymbol{y}|| \\leq ||\\boldsymbol{X}|| \\cdot ||\\boldsymbol{\\theta}|| $$\n",
    "\n",
    "Combining both inequalities:\n",
    "\n",
    "$$\n",
    "\\begin{gather*} \n",
    "\\dfrac{||\\delta \\boldsymbol{\\theta}||}{||\\boldsymbol{\\theta}||} \\leq ||\\boldsymbol{X}|| \\cdot ||\\boldsymbol{X}^{-1}|| \\cdot \\dfrac{||\\delta \\boldsymbol{y}||}{||\\boldsymbol{y}||}\\\\\n",
    "\\dfrac{||\\delta \\boldsymbol{\\theta}||}{||\\boldsymbol{\\theta}||} \\leq \\kappa(\\boldsymbol{X}) \\cdot \\dfrac{||\\delta \\boldsymbol{y}||}{||\\boldsymbol{y}||}\\\\\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "Where $\\kappa(\\boldsymbol{X})$ is condition number which  is defined as:  \n",
    "$$\\kappa(\\boldsymbol{X}) = ||\\boldsymbol{X}|| \\cdot ||\\boldsymbol{X}^{-1}|| = \\sqrt{\\dfrac{\\lambda_{\\max}(\\boldsymbol{X}^T\\boldsymbol{X})}{\\lambda_{\\min}(\\boldsymbol{X}^T\\boldsymbol{X})}} = \\dfrac{\\sigma_{max}(\\boldsymbol{X})}{\\sigma_{min}(\\boldsymbol{X})}$$\n",
    "\n",
    "##### 2. Now we consider simultaneous perturbations in $\\boldsymbol{X}$ and $\\theta$:\n",
    "$$\n",
    "\\begin{gather*}\n",
    "(\\boldsymbol{X} + \\delta\\boldsymbol{X})(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta}) = \\boldsymbol{y} \\\\\n",
    "\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta} = (\\boldsymbol{X} + \\delta\\boldsymbol{X})^{-1}\\boldsymbol{y} \\\\\n",
    "\\delta \\boldsymbol{\\theta} = (\\boldsymbol{X} + \\delta\\boldsymbol{X})^{-1}\\boldsymbol{y} -\\boldsymbol{\\theta}  \\\\\n",
    "\\delta \\boldsymbol{\\theta} = (\\boldsymbol{X} + \\delta\\boldsymbol{X})^{-1}\\boldsymbol{y} -\\boldsymbol{X}^{-1}\\boldsymbol{y}  \\\\\n",
    "\\delta \\boldsymbol{\\theta} = ((\\boldsymbol{X} + \\delta\\boldsymbol{X})^{-1} -\\boldsymbol{X}^{-1})\\boldsymbol{y}  \\\\\n",
    "\\delta \\boldsymbol{\\theta} = (\\boldsymbol{X}^{-1}(\\boldsymbol{X} - (\\boldsymbol{X} + \\delta\\boldsymbol{X}))(\\boldsymbol{X} + \\delta\\boldsymbol{X})^{-1})\\boldsymbol{y}  \\\\\n",
    "\\delta \\boldsymbol{\\theta} = -\\boldsymbol{X}^{-1}\\delta\\boldsymbol{X}(\\boldsymbol{X} + \\delta\\boldsymbol{X})^{-1})\\boldsymbol{y}  \\\\\n",
    "\\delta \\boldsymbol{\\theta} = -\\boldsymbol{X}^{-1}\\delta\\boldsymbol{X}(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta})  \\\\\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "That gives us:\n",
    "$$\n",
    "\\begin{gather*}\n",
    "||\\delta \\boldsymbol{\\theta}|| \\leq ||\\boldsymbol{X}^{-1}|| \\cdot ||\\delta\\boldsymbol{X}|| \\cdot ||(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta}) || \\\\\n",
    "\\dfrac{||\\delta \\boldsymbol{\\theta}||}{||(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta}) ||} \\leq ||\\boldsymbol{X}|| \\cdot ||\\boldsymbol{X}^{-1}|| \\cdot \\dfrac{||\\delta\\boldsymbol{X}||}{||\\boldsymbol{X}||}  \\\\\n",
    "\\dfrac{||\\delta \\boldsymbol{\\theta}||}{||(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta}) ||} \\leq \\kappa(\\boldsymbol{X}) \\cdot \\dfrac{||\\delta\\boldsymbol{X}||}{||\\boldsymbol{X}||}  \\\\\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "As we can see from both equations, condition number gives us the coefficient of the error bound. Fro example if condition number is 1000 and the relative perturbation of $\\dfrac{||\\delta \\boldsymbol{y}||}{||\\boldsymbol{y}||}$ is 0.1, than relative perturbation of $\\dfrac{||\\delta \\boldsymbol{\\theta}||}{||\\boldsymbol{\\theta}||} \\leq 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Matrix X: \n",
      "[[1.    2.   ]\n",
      " [2.    3.999]]\n",
      "y: \n",
      "[4.    7.999]\n",
      "\n",
      "singular values of X: 4.999e+00 2.000e-04\n",
      "cond number of X: 2.499e+04\n",
      "True solution theta: 2.000e+00 1.000e+00 \n",
      "X@theta= [4.    7.999]\n",
      "\n",
      "Now we add small perturbation of y:\n",
      "y_p = [4.001 7.998]\n",
      "Obtained new solution theta prime: -3.999e+00 4.000e+00\n",
      "\n",
      "perturbation of thetha: 3.000e+00 \n",
      "perturbation of y: 1.581e-04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def rnd(x, digits=3):\n",
    "    # Rounding of values for convinient printing:\n",
    "    return \" \".join([f\"{val :.3e}\" for val in x])\n",
    "\n",
    "# Input matrix X and vector y:\n",
    "X = np.array([[1, 2], [2, 3.999]])\n",
    "y = np.array([4, 7.999])\n",
    "print(f\" Matrix X: \\n{X}\")\n",
    "print(f\"y: \\n{y}\")\n",
    "print()\n",
    "\n",
    "# SVD decomposition of X and condition number calculation:\n",
    "u, s, vh = np.linalg.svd(X)\n",
    "cond_number = max(s)/min(s)\n",
    "print(f\"singular values of X: {rnd(s)}\")\n",
    "print(f\"cond number of X: {cond_number :.3e}\")\n",
    "# True solution vector theta:\n",
    "theta = np.array([2, 1])\n",
    "print(f\"True solution theta: {rnd(theta)} \\nX@theta= {X@theta}\")\n",
    "\n",
    "# Addition of small perturbation to y:\n",
    "print(f\"\\nNow we add small perturbation of y:\")\n",
    "y_p = y + np.array([0.001, -0.001]) \n",
    "print(f\"y_p = {y_p}\")\n",
    "# New solution after perturbation:\n",
    "theta_p = np.linalg.pinv(X).dot(y_p)\n",
    "print(f\"Obtained new solution theta prime: {rnd(theta_p)}\\n\")\n",
    "\n",
    "# Computing ||delta theta||/||theta|| and ||delta y||/||y||\n",
    "d_theta = theta_p - theta\n",
    "d_y = y_p - y\n",
    "perturbation_theta = np.linalg.norm(d_theta, ord=2)/np.linalg.norm(theta, ord=2)\n",
    "perturbation_y = np.linalg.norm(d_y, ord=2)/np.linalg.norm(y, ord=2)\n",
    "print(f\"perturbation of thetha: {perturbation_theta :.3e} \\nperturbation of y: {perturbation_y :.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see a small perturbation of $\\dfrac{||\\delta \\boldsymbol{y}||}{||\\boldsymbol{y}||} = 1.58e^{-3}$ , results in big perturbation in big perturbation of $\\dfrac{||\\delta \\boldsymbol{\\theta}||}{||\\boldsymbol{\\theta}||} = 3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variance Inflation Factor\n",
    "Mathematically, multicollinearity can be detected by calculating the variance inflation factor (VIF) for each independent variable. The VIF measures the inflation in the variance of the estimated regression coefficient due to multicollinearity. The formula for VIF is:\n",
    "\n",
    "$$ VIF(X_i) = \\frac{1}{1 - R_{i}^{2}} $$\n",
    "\n",
    "where $( R_{i}^{2} )$ is the coefficient of determination $( R^{2} )$ obtained by regressing the $ (i) $ th independent variable on all other independent variables. A VIF value greater than 1 indicates the presence of multicollinearity, and a higher VIF suggests a higher degree of multicollinearity.\n",
    "\n",
    "#### Coefficient of Determination (R-squared) for Linear Regression\n",
    "\n",
    "The coefficient of determination, often denoted as $ R^2 $, is a statistical measure used to assess the goodness of fit of a linear regression model. It quantifies the proportion of the variance in the dependent variable ($ y $) that can be explained by the independent variables ($ X $) in the model.\n",
    "\n",
    "\n",
    "The formula to calculate the coefficient of determination is given by:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\text{SSR}}{\\text{SST}} $$\n",
    "\n",
    "Where:\n",
    "- $ SSR $ (Sum of Squares Residuals) is the sum of squared differences between the observed values ($ y $) and the predicted values ($ \\hat{y} $) from the regression model.\n",
    "- $ SST $ (Total Sum of Squares) is the sum of squared differences between the observed values ($ y $) and the mean of the observed values ($ \\bar{y} $).\n",
    "\n",
    "\n",
    "The coefficient of determination $ R^2 $ ranges between 0 and 1. Here's how to interpret its values:\n",
    "\n",
    "- $ R^2 = 0 $: The model explains none of the variability in the dependent variable ($ y $).\n",
    "- $ 0 < R^2 < 1 $: The model explains some of the variability, with higher values indicating a better fit.\n",
    "- $ R^2 = 1 $: The model explains all of the variability in the dependent variable, and it perfectly predicts the observed values.\n",
    "\n",
    "\n",
    "While $ R^2 $ is a useful measure, it has limitations:\n",
    "- It can be misleading when applied to complex models with many predictors, as it tends to increase with the number of predictors even if they don't contribute much to the model's performance.\n",
    "- A high $ R^2 $ doesn't imply a causal relationship; it only indicates a strong linear relationship between the variables.\n",
    "\n",
    "**Adjusted R-squared**\n",
    "\n",
    "To account for the issue of adding more predictors, the adjusted $ R^2 $ is often used. The adjusted $ R^2 $ penalizes the inclusion of unnecessary predictors and provides a better measure of the model's fit.\n",
    "\n",
    "The formula for adjusted $ R^2 $ is:\n",
    "\n",
    "$$ \\text{Adjusted } R^2 = 1 - \\frac{\\frac{\\text{SSR}}{n - k - 1}}{\\frac{\\text{SST}}{n - 1}} $$\n",
    "\n",
    "Where:\n",
    "- $ n $ is the number of observations.\n",
    "- $ k $ is the number of predictors.\n",
    "\n",
    "Adjusted $ R^2 $ will be lower than $ R^2 $ if extra predictors do not improve the model's fit.\n",
    "\n",
    "\n",
    "\n",
    "The coefficient of determination $ R^2 $ provides insight into how well a linear regression model fits the data. It's a valuable tool for evaluating the quality of the regression model's predictions and understanding the extent to which the independent variables explain the variation in the dependent variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (5, 3)\n",
      "X: \n",
      "[[ 0.18111632  1.32039937  3.35043103]\n",
      " [ 3.6753899   5.16234369 10.05086423]\n",
      " [ 5.57339462  9.74381956 14.26883147]\n",
      " [ 7.11849266 15.45028466 19.94308465]\n",
      " [ 9.3744798  18.18077945 27.08932863]]\n",
      "\n",
      "VIF for X columns: [45.527609040316804, 38.15915760380514, 75.84121770500003]\n"
     ]
    }
   ],
   "source": [
    "## VIF CODE\n",
    "import numpy as np\n",
    "from mllib.linear_regression import LinearRegression\n",
    "\n",
    "\n",
    "def determination_coef(y_hat, y):\n",
    "    ssr = np.sum((y - y_hat)**2)\n",
    "    y_mean = np.mean(y)\n",
    "    sst = np.sum((y - y_mean)**2)\n",
    "    return 1 - ssr/sst\n",
    "\n",
    "def vif(X):\n",
    "    result = []\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        X = np.array(X)\n",
    "    for i in range(X.shape[1]):\n",
    "        non_i_cols = [col for col in range(X.shape[1]) if col != i]\n",
    "        X_temp = X[:, non_i_cols]\n",
    "        y_temp =X[:, i]\n",
    "        \n",
    "        y_hat = LinearRegression().fit(X_temp, y_temp).predict(X_temp)\n",
    "        r_squared_i = determination_coef(y_hat, y_temp)\n",
    "        # print(f\"idx: {i} r_squared_i: {r_squared_i}\")\n",
    "        vif = 1/ np.clip((1 - r_squared_i), 1e-16, None)\n",
    "        result.append(vif)\n",
    "    return result\n",
    "\n",
    "\n",
    "X = np.array([k*np.array([1, 3, 5, 7, 9]) for k in [1, 2, 3]]).astype(float).T  \n",
    "X += np.random.randn(*X.shape)\n",
    "print(f\"X shape: {X.shape}\\nX: \\n{X}\\n\")\n",
    "print(f\"VIF for X columns: {vif(X)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with L2 Regularization\n",
    "\n",
    "In linear regression with L2 regularization, the loss function is modified to include a penalty term that discourages large values of the coefficients ($ w $). This helps prevent overfitting and can lead to more stable solutions.\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The loss function for linear regression with L2 regularization is given by:\n",
    "\n",
    "$$\n",
    "J(\\boldsymbol{\\theta}) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i -  \\sum_{j=1}^{k}X_{i,j}θ_j)^2 + \\frac{\\lambda}{2} \\|\\boldsymbol{\\theta}\\|_2^2\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ n $ is the number of data points.\n",
    "- $ y_i $ is the observed value for data point $ i $.\n",
    "- $ \\boldsymbol{X} $ is the feature matrix with an added column of ones for the intercept term.\n",
    "- $ \\boldsymbol{\\theta} $ is the vector of coefficients (weights).\n",
    "- $ \\lambda $ is the regularization parameter (also known as the regularization strength).\n",
    "- $ \\|\\boldsymbol{\\theta}\\|_2^2 $ represents the L2 norm (Euclidean norm) of the coefficient vector.\n",
    "\n",
    "The first term in the loss function represents the ordinary least squares (OLS) loss, and the second term is the L2 regularization penalty.\n",
    "\n",
    "### Loss Derivative\n",
    "\n",
    "The gradient of the loss function with respect to the coefficient vector $ \\boldsymbol{\\theta} $ is:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\boldsymbol{\\theta}) =  \\boldsymbol{X}^T (\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}) + \\lambda \\boldsymbol{\\theta}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\boldsymbol{X} $ is the matrix of feature vectors for all data points.\n",
    "- $ \\boldsymbol{y} $ is the vector of observed values.\n",
    "\n",
    "The first term in the gradient corresponds to the gradient of the OLS loss, and the second term ($ \\lambda \\boldsymbol{\\theta} $) is the gradient of the L2 regularization term.\n",
    "\n",
    "### Closed form solution\n",
    "To find the closed-form solution, we aim to minimize the loss function by taking its derivative with respect to $ \\boldsymbol{\\theta} $ and setting it to zero:\n",
    "\n",
    "$$\n",
    "\\nabla J(\\boldsymbol{\\theta}) = 0\n",
    "$$\n",
    "\n",
    "Solving for $ \\boldsymbol{\\theta} $, we get:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "0 = \\boldsymbol{X}^T (\\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{y}) + \\lambda \\boldsymbol{\\theta} \\\\\n",
    "0 = \\boldsymbol{X}^T \\boldsymbol{X}\\boldsymbol{\\theta} - \\boldsymbol{X}^T\\boldsymbol{y} + \\lambda \\boldsymbol{\\theta} \\\\\n",
    "0 = (\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda I)\\boldsymbol{\\theta} - \\boldsymbol{X}^T\\boldsymbol{y} \\\\\n",
    "(\\boldsymbol{X}^T \\boldsymbol{X} + \\lambda I)\\boldsymbol{\\theta} = \\boldsymbol{X}^T\\boldsymbol{y} \\\\\n",
    "\\boldsymbol{\\theta} = (\\boldsymbol{X}^T\\boldsymbol{X} + \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{X}^T \\boldsymbol{y}\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "\n",
    "L2 regularization adds a penalty term to the linear regression loss function that encourages smaller coefficient values. This helps prevent overfitting and can improve the generalization ability of the model. The regularization parameter $ \\lambda $ controls the strength of the regularization, allowing you to find an optimal balance between fitting the data and controlling the complexity of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X features: 10\n",
      "True coefficients: \n",
      "  16.748    0.000    0.000   63.643    0.000   70.648    0.000   10.457    3.159    0.000\n",
      "Direct solution coefficients: \n",
      "  16.748    0.061    0.066   63.599    0.176   70.660   -0.098   10.326    3.195   -0.136    0.099\n",
      "SGD solution coefficients: \n",
      "  16.669   -0.356    0.547   63.464    0.514   69.977    0.559   10.911    3.098    0.568    0.223\n"
     ]
    }
   ],
   "source": [
    "## MLA CODE for LR and LR with L2\n",
    "from sklearn.datasets import make_regression\n",
    "from mllib.linear_regression import LinearRegression\n",
    "\n",
    "def rnd_list(x):\n",
    "    return \" \".join([f\"{val :8.3f}\" for val in x])\n",
    "\n",
    "X, y, coef = make_regression(n_samples=100, n_features=10, n_informative=5, noise=1, random_state=42, coef=True)\n",
    "print(f\"X features: {X.shape[1]}\")\n",
    "print(f\"True coefficients: \\n{rnd_list(coef)}\")\n",
    "mllib_lr = LinearRegression(alpha=0).fit(X, y)\n",
    "print(f\"Direct solution coefficients: \\n{rnd_list(mllib_lr.parameters)}\")\n",
    "mllib_lr_gd = LinearRegression(solver=\"sgd\", alpha=0).fit(X, y)\n",
    "print(f\"SGD solution coefficients: \\n{rnd_list(mllib_lr_gd.parameters)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links and resources:\n",
    "\n",
    "https://www.stat.cmu.edu/~larry/=stat401/lecture-17.pdf\n",
    "https://gregorygundersen.com/blog/2021/07/12/multicollinearity/\n",
    "https://stats.stackexchange.com/questions/479886/unexpected-relative-value-of-eigenvalues-of-a-top-a-and-a-top-a-1-in?noredirect=1&lq=1\n",
    "https://math.stackexchange.com/questions/1181271/if-ata-is-invertible-then-a-has-linearly-independent-column-vectors\n",
    "https://stats.stackexchange.com/questions/221902/what-is-an-example-of-perfect-multicollinearity\n",
    "https://mathformachines.com/posts/least-squares-with-the-mp-inverse/\n",
    "https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html#which-one-to-use-l1-or-l2\n",
    "\n",
    "**Books**\n",
    "\n",
    "Xian-Da Zhang, A Matrix Algebra Approach to Artificial Intelligence, 978-981-15-2769-2, 2020  \n",
    "Sheng Xu, Introduction to Scientific Computing with MATLAB® and Python Tutorials, CRC Press, 2022, ISBN\t1000596540, 9781000596540"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
