{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a foundational technique in statistics and machine learning that models the relationship between a dependent variable and one or more independent variables. It aims to find the best-fitting linear equation that predicts the dependent variable based on the independent variables. In this chapter, we will delve into the concepts of linear regression, its assumptions, mathematical representation, and different solution methods.\n",
    "\n",
    "## Assumptions of Linear Regression\n",
    "\n",
    "Before applying linear regression, it's important to consider its underlying assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between independent variables and the dependent variable is assumed to be linear.\n",
    "\n",
    "2. **Independence**: The residuals (differences between actual and predicted values) are independent of each other.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals is constant across all levels of the independent variables.\n",
    "\n",
    "4. **Normality**: The residuals follow a normal distribution.\n",
    "\n",
    "5. **No Multicollinearity**: The independent variables are not highly correlated with each other.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "Let's define the following terms for the mathematical formulation of linear regression:\n",
    "\n",
    "- **X**: The matrix of independent variables (features), with an added column of ones for the intercept term.\n",
    "- **y**: The vector of target values.\n",
    "- **θ**: The vector of coefficients (weights) that we want to optimize.\n",
    "- **n**: The number of data points.\n",
    "- **m**: The number of features.\n",
    "\n",
    "The extended linear regression equation can be written as:\n",
    "\n",
    "$$\n",
    "y = Xθ + ε\n",
    "$$\n",
    "\n",
    "where ε represents the error term.\n",
    "\n",
    "## Direct Solution: Normal Equation\n",
    "\n",
    "One way to find the optimal coefficients θ directly is by using the normal equation. The normal equation provides an analytical solution to the least squares problem, minimizing the sum of squared errors.\n",
    "\n",
    "The normal equation is:\n",
    "\n",
    "$$\n",
    "θ = (X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{X^T}$: The transpose of the feature matrix X.\n",
    "- $\\mathbf{y}$: The vector of target values.\n",
    "\n",
    "The normal equation directly calculates the optimal values of θ that minimize the loss function.\n",
    "\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "The goal of linear regression is to find the coefficients θ that minimize the difference between the predicted values (Xθ) and the actual target values (y). The most common loss function used is the Mean Squared Error (MSE):\n",
    "\n",
    "$$\n",
    "J(θ) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i -  \\sum_{j=1}^{k}X_{i,j}θ_j)^2\n",
    "$$\n",
    "\n",
    "where n is the number of data points, and X_i represents the i-th row of the feature matrix X.\n",
    "\n",
    "## Gradient Descent for Linear Regression\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize the loss function. The gradient of the loss function with respect to the coefficients θ is computed, and θ is updated in the opposite direction of the gradient. The update rule is:\n",
    "\n",
    "$$\n",
    "θ := θ - α \\nabla J(θ)\n",
    "$$\n",
    "\n",
    "where α is the learning rate.\n",
    "\n",
    "The gradient of the loss function can be computed as:\n",
    "\n",
    "$$\n",
    "\\nabla J(θ) = \\frac{1}{n} X^T(Xθ - y)\n",
    "$$\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Linear regression serves as a powerful tool for modeling relationships between variables. By considering its assumptions and understanding various solution methods, such as gradient descent and the normal equation, we can apply linear regression effectively to various prediction tasks. Its simplicity, interpretability, and versatility make linear regression a cornerstone of both statistical analysis and machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 0 3 4 1 5 2 7]\n",
      "[1 2 3 5 4 6 7 9]\n",
      "[(1, 6), (2, 0), (3, 3), (4, 1), (5, 4), (6, 5), (7, 2), (9, 7)]\n",
      "[6, 0, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([2, 4, 7, 3, 5, 6, 1, 9])\n",
    "ind = np.argpartition(x, 5)\n",
    "print(ind)\n",
    "print(x[ind])\n",
    "\n",
    "x_s = sorted([(val, ind) for ind, val in enumerate(x)], key= lambda x: x[0])\n",
    "print(x_s)\n",
    "print([x[1] for x in x_s[:3]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. LR Assumptions.\n",
    "2. LR Definition.\n",
    "3. LR Direct Solution.\n",
    "4. LR Gradient Methods.\n",
    "5. LR Multicoliniarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10, 2) y shape: (10,) X_intercept shape: (10, 3) theta shape: (3,)\n",
      " loss fn: 4958.580652311469\n",
      " grad fn: [ 10.84934975 -33.04821563 -44.95133608]\n",
      "Closed form: [-1.8251543  43.12541053 71.25877893] y_pred: [  16.59073627  -23.11103096  134.63592763  120.96550839    9.74330219\n",
      "  -54.9975661   -28.60742837 -141.62310256 -116.2808667  -127.72844599] y: [  20.03819923  -22.11994877  147.69902512  132.3912091    11.86498924\n",
      "  -57.32737267  -29.60728634 -150.45066604 -123.10948375 -139.7916313 ]\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "C = 1\n",
    "\n",
    "def loss_fn(X_intercept, y, parameters):\n",
    "    I = np.eye(parameters.shape[0])\n",
    "    I[0,0] = 0\n",
    "    n = y.shape[0]\n",
    "    return (1/(2*n))*(np.sum((X_intercept @ parameters - y)**2 + np.sum(C * np.concatenate(([0], parameters[1:]))**2)))\n",
    "\n",
    "def grad_fn(X_intercept, y, parameters):\n",
    "    I = np.eye(parameters.shape[0])\n",
    "    I[0,0] = 0\n",
    "    n = y.shape[0]\n",
    "    return (1/(2*n))*(X_intercept.T @ (X_intercept @ parameters - y) + C * np.concatenate(([0], parameters[1:])))\n",
    "\n",
    "def closed_form(X_intercept, y, parameters):\n",
    "    I = np.eye(parameters.shape[0])\n",
    "    I[0,0] = 0\n",
    "    return np.linalg.inv(X_intercept.T @ X_intercept + C*I) @ X_intercept.T @ y\n",
    "\n",
    "X, y = make_regression(n_samples=10, n_features=2, noise=1, random_state=42)\n",
    "\n",
    "\n",
    "X_intercept = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "theta = np.ones(X_intercept.shape[1])\n",
    "print(f\"X shape: {X.shape} y shape: {y.shape} X_intercept shape: {X_intercept.shape} theta shape: {theta.shape}\")\n",
    "\n",
    "print(f\" loss fn: {loss_fn(X_intercept, y, theta)}\")\n",
    "print(f\" grad fn: {grad_fn(X_intercept, y, theta)}\")\n",
    "solution = closed_form(X_intercept, y, theta)\n",
    "print(f\"Closed form: {solution} y_pred: {X_intercept @ solution} y: {y}\" )\n",
    "print(X_intercept.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(([0], theta[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression links:\n",
    "https://aunnnn.github.io/ml-tutorial/html/blog_content/linear_regression/linear_regression_regularized.html#which-one-to-use-l1-or-l2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of Linear Regression\n",
    "\n",
    "Before applying linear regression, it's important to consider its underlying assumptions:\n",
    "\n",
    "1. **Linearity**: The relationship between independent variables and the dependent variable is assumed to be linear.  \n",
    "\n",
    "2. **Independence**: The residuals (differences between actual and predicted values) are independent of each other.\n",
    "\n",
    "3. **Homoscedasticity**: The variance of the residuals is constant across all levels of the independent variables.\n",
    "\n",
    "4. **Normality**: The residuals follow a normal distribution.\n",
    "\n",
    "5. **No Multicollinearity**: The independent variables are not highly correlated with each other."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity:\n",
    "https://www.stat.cmu.edu/~larry/=stat401/lecture-17.pdf\n",
    "https://gregorygundersen.com/blog/2021/07/12/multicollinearity/\n",
    "https://stats.stackexchange.com/questions/479886/unexpected-relative-value-of-eigenvalues-of-a-top-a-and-a-top-a-1-in?noredirect=1&lq=1\n",
    "https://math.stackexchange.com/questions/1181271/if-ata-is-invertible-then-a-has-linearly-independent-column-vectors\n",
    "https://stats.stackexchange.com/questions/221902/what-is-an-example-of-perfect-multicollinearity\n",
    "https://mathformachines.com/posts/least-squares-with-the-mp-inverse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity in the context of linear regression refers to a situation where two or more predictor variables (features) in your dataset are highly correlated. This high correlation can lead to instability and issues when computing the direct solution of linear regression, $y = X\\theta$, where y is the target variable, $X$ is the design matrix of predictor variables, and $\\theta$ is the vector of coefficients that we want to estimate.\n",
    "\n",
    "When multicollinearity is present, it means that some predictor variables can be approximately expressed as a linear combination of other predictor variables. This can cause problems when trying to compute the pseudo-inverse of the matrix $X$, which is needed to calculate the least squares estimates of the coefficients $\\theta$.\n",
    "\n",
    "**Computation of pseudo-inverse of $X$**:\n",
    "- Direct computation:  \n",
    "\n",
    "    $X^+ = (X^TX)^{-1}X^T$\n",
    "\n",
    "- via SVD:\n",
    "$$ \n",
    "\\begin{gather*}\n",
    "X = U\\Sigma V^T \\\\\n",
    "X^+ = V \\Sigma^{-1} U^T\n",
    "\\end{gather*}\n",
    "$$\n",
    "Where $\\Sigma  = diag(\\sigma_1(X), \\sigma_2(X), \\cdots, \\sigma_n(X)) = diag(\\sqrt{\\lambda_1(X^TX)}, \\sqrt{\\lambda_2(X^TX)}, \\cdots, \\sqrt{\\lambda_n(X^TX)})$\n",
    "\n",
    "Since $rank(X^TX) = rank(X)$, if $rank(X_{m \\times n}) < \\min(m, n)$ than $X^TX$ is singular.  It means that some of its eigenvalues are zero or very close to zero.\n",
    "\n",
    "When performing matrix operations involving a singular or ill-conditioned matrix, numerical instability can occur. Small numerical errors can lead to large errors in the computed solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition number\n",
    "\n",
    "Ill-conditioning occurs when the condition number of the matrix $X^T * X$ is very large. The condition number is a measure of how sensitive the solution is to changes in the input data. A high condition number means that small changes in the input can lead to large changes in the output, making the solution unstable.\n",
    "\n",
    "Lets numerically evaluate the stability of the solution of $y = X\\theta$:\n",
    "\n",
    "We define:\n",
    "- $\\delta \\boldsymbol{\\theta}$: as perturbation of $\\boldsymbol{\\theta}$\n",
    "- $\\delta \\boldsymbol{y}$: as perturbation of $y$\n",
    "- $||\\boldsymbol{X}||$: as second matrix norm, $||\\boldsymbol{X}||_2 = \\max \\boldsymbol{\\sigma}(\\boldsymbol{X})$\n",
    "- $\\boldsymbol{\\sigma}(\\boldsymbol{X})$: singular values of matrix $\\boldsymbol{X}$\n",
    "\n",
    "$$ \\boldsymbol{X}(\\boldsymbol{\\theta} + \\delta \\boldsymbol{\\theta}) = \\boldsymbol{y} + \\delta \\boldsymbol{y} $$\n",
    "\n",
    "Since $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}$:\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\boldsymbol{X}\\boldsymbol{\\theta} + \\boldsymbol{X}\\delta \\boldsymbol{\\theta} = \\boldsymbol{y} + \\delta \\boldsymbol{y} \\\\\n",
    "\\boldsymbol{y} + \\boldsymbol{X}\\delta \\boldsymbol{\\theta} = \\boldsymbol{y} + \\delta \\boldsymbol{y} \\\\\n",
    "\\boldsymbol{X}\\delta \\boldsymbol{\\theta} = \\delta \\boldsymbol{y} \\\\\n",
    "\\delta \\boldsymbol{\\theta} = \\boldsymbol{X}^{-1} \\delta \\boldsymbol{y} \\\\\n",
    "||\\delta \\boldsymbol{\\theta}|| = ||\\boldsymbol{X}^{-1} \\delta \\boldsymbol{y}|| \\\\\n",
    "||\\delta \\boldsymbol{\\theta}|| \\leq ||\\boldsymbol{X}^{-1}||\\cdot||\\delta \\boldsymbol{y}|| \\\\\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "From $\\boldsymbol{y} = \\boldsymbol{X}\\boldsymbol{\\theta}$ we get:\n",
    "\n",
    "$$||\\boldsymbol{y}|| \\leq ||\\boldsymbol{X}|| \\cdot ||\\boldsymbol{\\theta}|| $$\n",
    "\n",
    "Combining both inequalities:\n",
    "\n",
    "$$ \\dfrac{||\\delta \\boldsymbol{\\theta}||}{||\\boldsymbol{\\theta}||} \\leq ||\\boldsymbol{X}|| \\cdot ||\\boldsymbol{X}^{-1}||  \\dfrac{||\\delta \\boldsymbol{y}||}{||\\boldsymbol{y}||}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Singular or Near-Singular Matrix:\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "When we perform linear regression using the least squares method, we aim to find the coefficients that minimize the residual sum of squares. This involves solving the equation $y = X\\theta$, where X is the design matrix of predictor variables, w is the vector of coefficients, and y is the target variable. One key step in this process is calculating the matrix product X^T * X, which is used to estimate the coefficients w.\n",
    "\n",
    "When multicollinearity is present, it means that some predictor variables are highly correlated, and this can lead to issues with the matrix X^T * X.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "The matrix X^T * X is used to compute the coefficients w as follows:\n",
    "\n",
    "w = (X^T * X)^(-1) * X^T * y\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's consider an example with two highly correlated predictor variables: height in feet and height in inches. These two variables are essentially representing the same information, and therefore they are linearly dependent, resulting in multicollinearity.\n",
    "\n",
    "Suppose our design matrix X is:\n",
    "\n",
    "X = [[5, 60],\n",
    "     [6, 72],\n",
    "     [5.5, 66]]\n",
    "\n",
    "Now, let's compute X^T * X:\n",
    "\n",
    "X^T * X = [[5^2 + 6^2 + 5.5^2, 5*60 + 6*72 + 5.5*66],\n",
    "           [5*60 + 6*72 + 5.5*66, 60^2 + 72^2 + 66^2]]\n",
    "\n",
    "As we can see, the first term in the diagonal, which represents the sum of squares of the first predictor (height in feet), is very similar to the second term, which represents the sum of squares of the second predictor (height in inches). This similarity indicates that the matrix is nearly singular.\n",
    "\n",
    "## Ill-Conditioning:\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "Ill-conditioning occurs when the condition number of the matrix X^T * X is very large. The condition number is a measure of how sensitive the solution is to changes in the input data. A high condition number means that small changes in the input can lead to large changes in the output, making the solution unstable.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "The condition number (κ) of a matrix A is defined as the ratio of the largest to the smallest singular value of A:\n",
    "\n",
    "κ(A) = σ_max(A) / σ_min(A)\n",
    "\n",
    "where σ_max(A) is the largest singular value and σ_min(A) is the smallest singular value of matrix A.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Continuing from the previous example, let's compute the condition number of the matrix X^T * X:\n",
    "\n",
    "Assuming the computed X^T * X is:\n",
    "\n",
    "X^T * X = [[100, 600],\n",
    "           [600, 15000]]\n",
    "\n",
    "The singular values of this matrix are approximately 15018.8 and 81.2. Therefore, the condition number is:\n",
    "\n",
    "κ(X^T * X) = 15018.8 / 81.2 ≈ 185.03\n",
    "\n",
    "This high condition number indicates that the solution (coefficients w) will be highly sensitive to small changes in the input data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Inflation Factor\n",
    "Mathematically, multicollinearity can be detected by calculating the variance inflation factor (VIF) for each independent variable. The VIF measures the inflation in the variance of the estimated regression coefficient due to multicollinearity. The formula for VIF is:\n",
    "\n",
    "$$ VIF(X_i) = \\frac{1}{1 - R_{i}^{2}} $$\n",
    "\n",
    "where $( R_{i}^{2} )$ is the coefficient of determination $( R^{2} )$ obtained by regressing the $ (i) $ th independent variable on all other independent variables. A VIF value greater than 1 indicates the presence of multicollinearity, and a higher VIF suggests a higher degree of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (4, 4)\n",
      "[[ 2.    2.5   3.    3.5 ]\n",
      " [ 5.    6.25  7.5   8.75]\n",
      " [ 7.    8.75 10.5  12.25]\n",
      " [ 8.   10.   12.   14.  ]]\n",
      "egenvals: [0.00000000e+00+0.0000000e+00j 3.27500000e+01+0.0000000e+00j\n",
      " 5.97687542e-16+6.7382728e-16j 5.97687542e-16-6.7382728e-16j]\n",
      "singular vals: [3.34402452e+01 2.32995685e-15 7.74888563e-16 9.41612800e-32]\n",
      "cond number: 3.5513796361461816e+32\n",
      "[[ 2.01  2.5   3.    3.5 ]\n",
      " [ 5.    6.26  7.5   8.75]\n",
      " [ 7.    8.75 10.51 12.25]\n",
      " [ 8.   10.   12.   14.01]]\n",
      "egenvals: [1.000e-02 3.276e+01 1.000e-02 1.000e-02]\n",
      "singular vals: [3.34500389e+01 1.00000000e-02 1.00000000e-02 9.79371058e-03]\n",
      "cond number: 3415.461245676248\n",
      "[[ 2.1   2.5   3.    3.5 ]\n",
      " [ 5.    6.35  7.5   8.75]\n",
      " [ 7.    8.75 10.6  12.25]\n",
      " [ 8.   10.   12.   14.1 ]]\n",
      "egenvals: [ 0.1  32.85  0.1   0.1 ]\n",
      "singular vals: [33.53819325  0.1         0.1         0.09794803]\n",
      "cond number: 342.40803841192354\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "v = np.array([2, 5, 7, 8])\n",
    "X = np.stack([v*coef for coef in [1, 1.25, 1.5, 1.75]], axis=1)\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(X)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(X)\n",
    "u, s, vh = np.linalg.svd(X)\n",
    "print(f\"egenvals: {eigenvalues}\")\n",
    "print(f\"singular vals: {s}\")\n",
    "print(f\"cond number: {max(s)/min(s)}\")\n",
    "\n",
    "X = np.stack([v*coef for coef in [1, 1.25, 1.5, 1.75]], axis=1)\n",
    "X_p = 0.01*np.eye(4)\n",
    "X = X + X_p\n",
    "print(X)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(X)\n",
    "u, s, vh = np.linalg.svd(X)\n",
    "print(f\"egenvals: {eigenvalues}\")\n",
    "print(f\"singular vals: {s}\")\n",
    "print(f\"cond number: {max(s)/min(s)}\")\n",
    "\n",
    "X = np.stack([v*coef for coef in [1, 1.25, 1.5, 1.75]], axis=1)\n",
    "X_p = 0.1*np.eye(4)\n",
    "X = X + X_p\n",
    "print(X)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(X)\n",
    "u, s, vh = np.linalg.svd(X)\n",
    "print(f\"egenvals: {eigenvalues}\")\n",
    "print(f\"singular vals: {s}\")\n",
    "print(f\"cond number: {max(s)/min(s)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1.500000\n"
     ]
    }
   ],
   "source": [
    "a = [1, 1, 1]\n",
    "b = [2, 2, 2]\n",
    "\n",
    "print(sum(map(lambda x: x[0]>x[1], zip(a,b))))\n",
    "print(f\"{3/2 :.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1st</th>\n",
       "      <th>2nd</th>\n",
       "      <th>3rd</th>\n",
       "      <th>Crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>survived</th>\n",
       "      <td>203</td>\n",
       "      <td>118</td>\n",
       "      <td>178</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not survived</th>\n",
       "      <td>122</td>\n",
       "      <td>167</td>\n",
       "      <td>528</td>\n",
       "      <td>673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              1st  2nd  3rd  Crew\n",
       "survived      203  118  178   212\n",
       "not survived  122  167  528   673"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {\"1st\":[203, 122], \"2nd\":[118, 167], \"3rd\":[178, 528], \"Crew\":[212, 673]}\n",
    "df = pd.DataFrame(data=d, index=[\"survived\", \"not survived\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 2201 1st: 325 survived: 711 not survived: 1490\n",
      "P first class: 0.15\n",
      "P survived: 0.32\n",
      "P 1st class survived: 0.62\n",
      "0.26 0.26\n",
      "0.78 0.78\n",
      "0.45 0.45\n"
     ]
    }
   ],
   "source": [
    "total_psg = df.sum().sum()\n",
    "fst_class = df[\"1st\"].sum()\n",
    "survived = df.loc[\"survived\", :].sum()\n",
    "not_survived = df.loc[\"not survived\", :].sum()\n",
    "\n",
    "print(f\"total: {total_psg} 1st: {fst_class} survived: {survived} not survived: {not_survived}\")\n",
    "print(\"P first class:\", round(fst_class/total_psg, 2))\n",
    "print(\"P survived:\", round(survived/total_psg, 2))\n",
    "print(\"P 1st class survived:\", round(df.loc[\"survived\", \"1st\"]/fst_class, 2))\n",
    "print(round((3/5 +1/6)/3,2), round(23/90, 2))\n",
    "print(round((3/5)*(1/3)/(23/90),2), round(18/23, 2))\n",
    "print(round((1/3)/(67/90),2), round(30/67, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: [0.5 0.5]\n",
      "probabilities squared: [0.25 0.25]\n",
      "gini inpurity: 0.5\n",
      "===================\n",
      "probabilities: [0.5 0.5]\n",
      "log probabilities: [-1. -1.]\n",
      "p*log_2 p: [-0.5 -0.5]\n",
      "entropy inpurity: 1.0\n",
      "\n",
      "probabilities: [0.22222222 0.77777778]\n",
      "probabilities squared: [0.04938272 0.60493827]\n",
      "gini inpurity: 0.345679012345679\n",
      "===================\n",
      "probabilities: [0.22222222 0.77777778]\n",
      "log probabilities: [-2.169925   -0.36257008]\n",
      "p*log_2 p: [-0.48220556 -0.28199895]\n",
      "entropy inpurity: 0.7642045065086203\n",
      "\n",
      "probabilities: [0. 1.]\n",
      "probabilities squared: [0. 1.]\n",
      "gini inpurity: 0.0\n",
      "===================\n",
      "probabilities: [0. 1.]\n",
      "log probabilities: [0. 0.]\n",
      "p*log_2 p: [0. 0.]\n",
      "entropy inpurity: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gini_inpurity(y):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    probabilities = np.bincount(y)/y.shape[0]\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    print(f\"probabilities squared: {probabilities**2}\")\n",
    "    print(f\"gini inpurity: {1 - np.sum(probabilities**2)}\")\n",
    "    return 1 - np.sum(probabilities**2)\n",
    "\n",
    "def entropy_inpurity(y):\n",
    "    if not isinstance(y, np.ndarray):\n",
    "        y = np.array(y)\n",
    "    probabilities = np.bincount(y)/y.shape[0]\n",
    "    print(f\"probabilities: {probabilities}\")\n",
    "    print(f\"log probabilities: { np.log2(probabilities, where=(probabilities > 0))}\")\n",
    "    print(f\"p*log_2 p: { probabilities*np.log2(probabilities, where=(probabilities > 0))}\")\n",
    "    print(f\"entropy inpurity: {np.sum(-probabilities * np.log2(probabilities, where=(probabilities > 0)))}\")\n",
    "    return np.sum(-probabilities * np.log2(probabilities, where=(probabilities > 0)))\n",
    "\n",
    "y = [0, 0, 1, 1]\n",
    "gini_inpurity(y)\n",
    "print(\"===================\")\n",
    "entropy_inpurity(y)\n",
    "print()\n",
    "y = [0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "gini_inpurity(y)\n",
    "print(\"===================\")\n",
    "entropy_inpurity(y)\n",
    "\n",
    "print()\n",
    "y = [1, 1, 1, 1, 1, 1, 1]\n",
    "gini_inpurity(y)\n",
    "print(\"===================\")\n",
    "entropy_inpurity(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "probabilities = np.array([0, 1])\n",
    "print(np.unique(probabilities))\n",
    "entropy = np.sum(-probabilities * np.log2(probabilities, where=(probabilities > 0)))\n",
    "print(entropy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine an experiment with $k$ possible output categories. Category $j$ has a probability of occurrence $p(j|t)$ (where $j=1,\\ldots,k$).\n",
    "\n",
    "Reproduce the experiment two times and make these observations:\n",
    "\n",
    "1. The probability of obtaining two identical outputs of category $j$ is $p^2(j|t)$.\n",
    "2. The probability of obtaining two identical outputs, independently of their category, is: $\\sum_{j=1}^{k} p^2(j|t)$.\n",
    "3. The probability of obtaining two different outputs is thus: $1 - \\sum_{j=1}^{k} p^2(j|t)$.\n",
    "\n",
    "That's it: the Gini impurity is simply the probability of obtaining two different outputs, which is an \"impurity measure\".\n",
    "\n",
    "https://stats.stackexchange.com/questions/308885/a-simple-clear-explanation-of-the-gini-impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0 number: 1 reminder: {1: {0}}\n",
      "i: 1 number: 3 reminder: {1: {0}, 0: {1}}\n",
      "i: 2 number: 2 reminder: {1: {0}, 0: {1}, 2: {2}}\n",
      "i: 3 number: 6 reminder: {1: {0}, 0: {1, 3}, 2: {2}}\n",
      "i: 4 number: 1 reminder: {1: {0, 4}, 0: {1, 3}, 2: {2}}\n",
      "i: 5 number: 2 reminder: {1: {0, 4}, 0: {1, 3}, 2: {2, 5}}\n",
      "(0,2), (0,5)\n",
      "(1,3)\n",
      "(2,4)\n",
      "\n",
      "(4,5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def divisibleSumPairs(n, k, ar):\n",
    "    reminder = dict()\n",
    "    for i, number in enumerate(ar):\n",
    "        reminder.setdefault(number%k, set()).update([i])\n",
    "        print(f\"i: {i} number: {number} reminder: {reminder}\")\n",
    "    pairs_count = 0\n",
    "    for i in range(n-1):\n",
    "        rem_i = ar[i]%k\n",
    "        pairs = reminder.get((k-rem_i)%k)\n",
    "        #print(f\"i: {i} pairs: {pairs}\")\n",
    "        if pairs:\n",
    "            subset = pairs-set(list(range(i+1)))\n",
    "            print(\", \".join([f\"({i},{p})\" for p in subset]))\n",
    "            pairs_count += len(subset)\n",
    "    return pairs_count\n",
    "\n",
    "n = 6\n",
    "k = 3\n",
    "ar = [1, 3, 2, 6, 1, 2]\n",
    "divisibleSumPairs(n, k, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 1]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "y = np.array([0,0,1,2,2,2,3])\n",
    "print(np.bincount(y))\n",
    "print(np.argmax(np.bincount(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n",
      "inside while\n",
      "out of while\n"
     ]
    }
   ],
   "source": [
    "def introTutorial(V, arr):\n",
    "    # Write your code here\n",
    "    n = len(arr)\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    left = 0\n",
    "    right = n - 1\n",
    "    iter = 0\n",
    "    while (iter< 100) and (left != right):\n",
    "        iter += 1\n",
    "        ind = left + (right-left)//2\n",
    "        if arr[ind] == V:\n",
    "            return ind\n",
    "        elif arr[ind] > V:\n",
    "            right = ind-1\n",
    "        else:\n",
    "            left = ind+1\n",
    "    return left\n",
    "\n",
    "s = [1, 5, 51, 53, 2, 123, 223, 345, 5432, 111]\n",
    "for n in range(15):\n",
    "    for j in range(n):\n",
    "        s = list(range(n))\n",
    "        v = s[j]\n",
    "        res = introTutorial(v, s)\n",
    "        assert j == res, f\"fail. j:{j} ans: {res}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression links:  \n",
    "\n",
    " - https://ai.plainenglish.io/l1-lasso-and-l2-ridge-regularizations-in-logistic-regression-53ab6c952f15  \n",
    " - https://ml-explained.com/blog/logistic-regression-explained"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
